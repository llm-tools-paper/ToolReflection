{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from pprint import pprint\n",
    "from agent.custom_parser import CustomMRKLOutputParser2, CustomMRKLOutputParser, CustomMRKLOutputParser3\n",
    "from copy import deepcopy\n",
    "from utils import load_openapi_spec, escape\n",
    "from agent.tools import Tool, GetDetailsTool, tool_projection\n",
    "from agent.tools import CustomInvalidTool\n",
    "from transformers import StoppingCriteria\n",
    "import torch\n",
    "from langchain.schema import AgentAction, AgentFinish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../data/train_data_train_part', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8920e2b9b0f459cb3ae991a54ef6c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_dir = '/data6/polyakov/ToolAlpaca/vicuna-7b-toolalpaca_train_on_train_part/checkpoint-54'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_dir, trust_remote_code=True).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopSequenceCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_sequences, tokenizer):\n",
    "        if isinstance(stop_sequences, str):\n",
    "            stop_sequences = [stop_sequences]\n",
    "        self.stop_sequences = stop_sequences\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs) -> bool:\n",
    "        decoded_output = self.tokenizer.decode(input_ids.tolist()[0])\n",
    "        return any(\n",
    "            decoded_output.endswith(stop_sequence)\n",
    "            for stop_sequence in self.stop_sequences\n",
    "        )\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    device='cuda',\n",
    "    do_sample=False,\n",
    "    stopping_criteria=[StopSequenceCriteria('ASSISTANT Observation:', tokenizer)]\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/2261 [00:00<?, ?it/s]/data2/polyakov/anaconda3/envs/alpaca-lora/lib/python3.10/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "  0%|▏                                                            | 5/2261 [00:56<5:58:18,  9.53s/it]/data2/polyakov/anaconda3/envs/alpaca-lora/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "  1%|▎                                                           | 12/2261 [02:18<7:31:11, 12.04s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2532 > 2048). Running this sequence through the model will result in indexing errors\n",
      " 19%|██████████▉                                              | 436/2261 [1:48:38<7:35:36, 14.98s/it]"
     ]
    }
   ],
   "source": [
    "generations = []\n",
    "for item in tqdm(data):\n",
    "    curr_generation = deepcopy(item)\n",
    "    need_to_train = item[1]\n",
    "    curr_prompt = ''\n",
    "    prompts = []\n",
    "    for idx, step in enumerate(item[0]):\n",
    "        if need_to_train[idx]:\n",
    "            prompts.append(curr_prompt)\n",
    "        curr_prompt += step\n",
    "    res = llm.generate(prompts)\n",
    "    generation_idx = 0\n",
    "    for idx in range(len(curr_generation[0])):\n",
    "        if curr_generation[1][idx]:\n",
    "            curr_generation[0][idx] = res.generations[generation_idx][0].text.split('\\nASSISTANT Observation:')[0]\n",
    "            generation_idx += 1\n",
    "    generations.append(curr_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(generations, open('../data/test_generations_train_model_trained_on_half_train.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2261"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
